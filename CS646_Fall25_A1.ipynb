{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JianxinLin28/Attendify/blob/main/CS646_Fall25_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your name:**\n",
        "\n",
        "**Your student ID number:**\n",
        "\n",
        "**Shared link to this notebook:**"
      ],
      "metadata": {
        "id": "nRGQQT6lU1WG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LGeJzZQp8IP"
      },
      "source": [
        "#[COMPSCI 646: Information Retrieval - Fall 2025 ](https://umamherst.instructure.com/courses/29086)\n",
        "#Assignment 1: BM25 retrieval model (Total : 65 points + 10 Max extra)\n",
        "\n",
        "**Description**\n",
        "\n",
        "This assignment is focused on indexing and document retrieval on a small collection of documents and utilize the retrieved results to augment LLM generation (RAG). Basic proficiency in Python is recommended.  \n",
        "\n",
        "**Instructions**\n",
        "\n",
        "* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n",
        "\n",
        "* Next, open the copy in your Google Drive and close the original. This way, you'll be working on your own version and won't accidentally waste time on a copy that can't be saved.\n",
        "\n",
        "*   You can download this notebook (*`File -> Download -> Download .ipynb`*) and run it on your local machine, then upload the result file to Google Colab.\n",
        "\n",
        "* The following instructions assume you are working in your own copy of the notebook.\n",
        "\n",
        "*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n",
        "\n",
        "*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n",
        "\n",
        "\n",
        "\n",
        "**Submission Details**\n",
        "\n",
        "* Due date: **Sep. 29, 5pm ET**\n",
        "\n",
        "* Before starting the interesting part of this assignment, click the *Share* button at the top right of the Colab window. Make sure you use the link for your copy in Google Drive (not the link for the original notebook). Then, go to the first text box above, double click on it, and enter your name, student ID number, and the link (URL) to *your copy* of this notebook.\n",
        "\n",
        "* To create the final PDF submission file, use *`File -> Print -> Save as PDF`*. Make sure that the generated PDF contains all the codes and printed outputs before submission. You are responsible for uploading the correct PDF with all the information required for grading.\n",
        "\n",
        "* To create the final Python submission file, click on *`File -> Download -> Download .py`*.\n",
        "\n",
        "* Upload the PDF and Python files to [Gradescope](https://www.gradescope.com/courses/1047975) in the assignment **A1**.\n",
        "\n",
        "\n",
        "**Academic Honesty**\n",
        "\n",
        "Please follow the guidelines under the *Collaboration and Help* section of the slides about course policy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prelude\n",
        "\n",
        "In this section, we provide starter code to install and load the required libraries and dataset. The notebook is expected to run on Google Colab and thus all the instruction is written using Colab. Note that it can also work on your local machine with small tweaks on the parameters."
      ],
      "metadata": {
        "id": "ibNoctBxtu9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we load the base package and set up the required variables for the assignment."
      ],
      "metadata": {
        "id": "vMyd7JvNvDix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MHwP1evIr23"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "sys.displayhook = lambda x: None # Suppress notebook auto printout\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    in_colab = True\n",
        "except ImportError:\n",
        "    in_colab = False\n",
        "\n",
        "store_local = True # 'True' if you'd like to store the datasets and other files into the Google drive.\n",
        "\n",
        "\n",
        "if in_colab:\n",
        "    # Please allow the access to your Google Drive or the following dataset loader will fail.\n",
        "    drive.mount(\"/content/drive/\")  ## DO NOT MODIFY THIS LINE\n",
        "\n",
        "    if store_local:\n",
        "        # Store all assignment related contents into the google drive (persist across colab sessions)\n",
        "        data_path = \"/content/drive/MyDrive/COMPSCI646-F25/A1\"  ## Suggest to store all assignment related contents into one folder\n",
        "    else:\n",
        "        # Store all assignment related contents into the VM's per-session storage (do not persist across colab sessions)\n",
        "        data_path = \"/content/\"  ## Suggest to store all assignment related contents into one folder\n",
        "else:\n",
        "    # Store all assignment related contents into the local storage\n",
        "    data_path = \"./data/\"  ## Suggest to store all assignment related contents into one folder\n",
        "\n",
        "\n",
        "\n",
        "assert os.path.exists(\n",
        "    data_path\n",
        "), \"Change data_path to a valid and existing file path!\"\n",
        "\n",
        "\n",
        "# Configure the dataset filename and storage location. DO NOT MODIFY!!\n",
        "file_info_dict = {\n",
        "    \"corpus\": \"1OC3duSpoxnMKveES6tcYX42dXmTk_d0r\",\n",
        "    \"queries\": \"1AUOf1x7HDkdil5QJFTx-VB2fR_rgL-hZ\",\n",
        "    \"qrels\": \"13FEIGGt9Ick-kQvMrWRAeDbPvIvF6MCe\",\n",
        "}\n",
        "\n",
        "# Feel free to edit the path variable mentioned below, but make sure the target directory exists.\n",
        "## location for dataset contents\n",
        "corpus_zip_path = os.path.join(data_path, \"corpus.tsv.gz\")\n",
        "corpus_path = corpus_zip_path.rstrip(\".gz\")\n",
        "queries_path = os.path.join(data_path, \"queries.jsonl\")\n",
        "qrels_path = os.path.join(data_path, \"qrels.json\")\n",
        "\n",
        "## cache dir for Section 1\n",
        "jsonl_dir = os.path.join(data_path, \"jsonl_collections\") # storing the converted corpus entries\n",
        "index_dir = os.path.join(data_path, \"index\") #  storing the index components from pyserini\n",
        "\n",
        "## cache dir and path for Section 4\n",
        "json_cache_path = os.path.join(data_path, 'hotpotqa_dev_bm25.json') # storing the prepared entries for the LLM prompts\n",
        "output_dir = os.path.join(data_path, \"outputs\") # storing the output from the LLM inference\n",
        "\n",
        "\n",
        "\n",
        "# You are more than welcome to code some helper functions.\n",
        "# But do note that we are only grading functions that are coded in the template files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG42Wnd0bmez"
      },
      "source": [
        "##0.1.  Setup Packages\n",
        "\n",
        "In this assignment, we will use the following libraries to index the corpus and perform retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d8AEs2XrBqa"
      },
      "outputs": [],
      "source": [
        "### Pyserini require updating java version ###\n",
        "!apt-get install openjdk-21-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGdmtsKax9eY"
      },
      "outputs": [],
      "source": [
        "# !pip install --ignore-installed pyserini\n",
        "!pip install pyserini\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install datasets\n",
        "!pip install pytrec_eval\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0.2 Download the dataset\n",
        "\n",
        "In this assignment, we will use a subset of the HotpotQA dataset of [KILT](https://ai.meta.com/tools/kilt/): [a Benchmark for Knowledge Intensive Language Tasks](https://arxiv.org/pdf/2009.02252).\n",
        "\n",
        "\n",
        "**Corpus**\n",
        "\n",
        "We sample 10% of the original KILT collection of passages split into 100 words. In total, the sampled corpus contains 3,567,807 passages.\n"
      ],
      "metadata": {
        "id": "Oj5G0jXI8rtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Queries**\n",
        "\n",
        "For saving time, we sample the original query sets of HotpotQA, you can access them from [here](https://drive.google.com/file/d/1AUOf1x7HDkdil5QJFTx-VB2fR_rgL-hZ/view?usp=drive_link).\n",
        "\n"
      ],
      "metadata": {
        "id": "DQx_eykSHTNE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOExtraCnZP9"
      },
      "source": [
        "**Relevance judgment**\n",
        "\n",
        "For a query (input field of KILT), all passages in the collection that have a ```wikipedia_id``` available in the corresponding ```provenance``` field of ```input``` are considered relevant.\n",
        "\n",
        "<!-- Note that the ```output``` part of the KILT tasks contains several outputs for the ```input```, and each of them has a ```provenance``` field. The provenance field has a list of features (```wikipedia_id```, ```start_paragraph_id```, ```end_paragraph_id```) representing Wikipedia paragraphs that support the answer. The collection from the [TSV file](#tsv-file) includes ```id, text, wikipedia_title, wikipedia_id``` fields. -->\n",
        "\n",
        "You can access the relevance judment file (qrel file) [here](https://drive.google.com/file/d/13FEIGGt9Ick-kQvMrWRAeDbPvIvF6MCe/view?usp=drive_link).\n",
        "This file is created using information from the KILT tasks and the collection file.\n",
        "\n",
        "The qrel file has the following format:\n",
        "```\n",
        "qrel = {\n",
        "    'query_id_1': {\n",
        "        'passage_id_1': 1\n",
        "    },\n",
        "    'query_id2': {\n",
        "        'passage_id_10': 1,\n",
        "        'passage_id_99': 1,\n",
        "    },\n",
        "}\n",
        "```\n",
        "\n",
        "An example line from the file is:\n",
        "```\n",
        "{\n",
        "      \"6915606477668963399\": {\n",
        "            \"3894652\": 1,\n",
        "            \"3894653\": 1,\n",
        "            \"3894654\": 1,\n",
        "            \"3894655\": 1,\n",
        "            \"3894656\": 1,\n",
        "            \"3894657\": 1,\n",
        "            \"3894658\": 1\n",
        "      },\n",
        "...\n",
        "}\n",
        "```\n",
        "which shows passages \"3894652\", \"3894653\", \"3894654\", \"3894655\", \"3894656\", \"3894657\", and \"3894658\" from the TSV collection are relevant to the query \"6915606477668963399\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewpPJ6d8zDFO"
      },
      "source": [
        "<a name=\"tsv-file\"></a>\n",
        "**Download**\n",
        "\n",
        "You can use the code below to download the required files, including sampled KILT Wikipedia corpus, a subset of HotpotQA queries, and the corresponding qrels. The files are saved at `corpus_path`, `queries_path`, and `qrels_path`, which you can use in the rest of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLx2FKpP_jKF"
      },
      "outputs": [],
      "source": [
        "def download_file(drive_file_id: str, output_file_path: str):\n",
        "    \"\"\"\n",
        "    Download necessary from remote.\n",
        "\n",
        "    Args:\n",
        "        drive_file_id: the google drive file id\n",
        "        output_file_path: the location of file for local storage\n",
        "\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    remote_file_path = f\"https://drive.google.com/uc?export=download&id={drive_file_id}\"\n",
        "    if not os.path.isfile(output_file_path):\n",
        "        print(f'Cannot find \"{output_file_path}\" at \"{data_path}\" so downloading it...')\n",
        "        if output_file_path.endswith(\".gz\"):\n",
        "            import gdown\n",
        "            gdown.download(remote_file_path, output_file_path, quiet=False)\n",
        "        else:\n",
        "            import urllib.request\n",
        "            urllib.request.urlretrieve(remote_file_path, output_file_path)\n",
        "        print(\"Download complete!\")\n",
        "    else:\n",
        "        print(f'File \"{output_file_path}\" already exists, not downloading.')\n",
        "\n",
        "    if output_file_path.endswith(\".gz\"):\n",
        "        unzip_path = output_file_path.rstrip(\".gz\")\n",
        "        if os.path.isfile(unzip_path):\n",
        "            return\n",
        "\n",
        "        import gzip\n",
        "        import shutil\n",
        "        # Decompress with gzip\n",
        "        print(f'Unzipping \"{output_file_path}\" to \"{unzip_path}\"...')\n",
        "        try:\n",
        "            with gzip.open(output_file_path, \"rb\") as f_in, open(unzip_path, \"wb\") as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decompression: {e}\")\n",
        "            # Clean up both files if they exist\n",
        "            for path in [output_file_path, unzip_path]:\n",
        "                if os.path.exists(path):\n",
        "                    try:\n",
        "                        os.remove(path)\n",
        "                        print(f\"Removed {path}...\")\n",
        "                    except Exception as rm_e:\n",
        "                        print(f\"Failed to remove {path}: {rm_e}..\")\n",
        "            raise  # re-raise the exception\n",
        "        else:\n",
        "            print(\"Decompression complete!\")\n",
        "        print(\"Done!\")\n",
        "\n",
        "\n",
        "# Download Corpus\n",
        "download_file(drive_file_id=file_info_dict[\"corpus\"], output_file_path=corpus_zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Queries\n",
        "download_file(drive_file_id=file_info_dict[\"queries\"], output_file_path=queries_path)\n",
        "\n",
        "# Download Qrels\n",
        "download_file(drive_file_id=file_info_dict[\"qrels\"], output_file_path=qrels_path)"
      ],
      "metadata": {
        "id": "JeVgdBxAKRtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also provide the code to load the queries and qrels into the variables `raw_queries` and `qrels`."
      ],
      "metadata": {
        "id": "7OAI5Tf3vJk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "raw_queries = []\n",
        "with open(queries_path, \"r\", encoding=\"utf-8\") as f_in:\n",
        "  for line in f_in:\n",
        "    raw_queries.append(json.loads(line))\n",
        "\n",
        "with open(qrels_path) as f:\n",
        "  qrels = json.load(f)\n",
        "\n",
        "sample_query_id = \"5ae234385542992decbdcc59\"\n",
        "print(f\"The number of queries: {len(raw_queries)}\")\n",
        "print(\"Query Format:\\n\" + json.dumps(raw_queries[0], indent=4))\n",
        "print(\"Qrels Format:\\n\" + json.dumps({sample_query_id: qrels[sample_query_id]}, indent=4))"
      ],
      "metadata": {
        "id": "g9BVxUoGvIXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpVnuPRUrNiF"
      },
      "source": [
        "# 1. Indexing (20 Points)\n",
        "\n",
        "\n",
        "In this assigment, you will use the [Pyserini](https://github.com/castorini/pyserini/) toolkit to perform BM25 retrieval on the HotpotQA dataset.\n",
        "\n",
        "The first step is to index the document collection of the HotpotQA dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvzfM0yYr67F"
      },
      "source": [
        "## 1.1 Preprocessing the Dataset (10 points)\n",
        "\n",
        "To index a collection using Pyserini, the first step is to prepare the collection in an appropriate format. A simple format for Pyserini indexing is JSONL files that have two mandatory keys:\n",
        "\n",
        "```\n",
        "{\"id\": \"doc1\", \"contents\": \"this is the first assignment.\"}\n",
        "```\n",
        "\n",
        "You need to convert the downloaded TSV file into JSONL files.\n",
        "For this assignment, you only need the four following fields of the TSV files: ```id, text, wikipedia_title, wikipedia_id```.\n",
        "  \n",
        "The converted jsonl files for indexing consist of multiple lines in this format:\n",
        "\n",
        "```\n",
        "{\"id\": \"1000_0\", \"contents\": \"The first passage\", \"wikipedia_id\": \"99\"}\n",
        "{\"id\": \"1000_1\", \"contents\": \"The second passage\", \"wikipedia_id\": \"99\"}\n",
        "...\n",
        "```\n",
        "where ```id``` is the id of the passage, ```contents``` is a text that combine ```wikipedia_title``` and ```text``` fields of the TSV file, and ```wikipedia_id``` is the id of the Wikipedia document that contains the passage.\n",
        "\n",
        "<!-- You need to implement the ```convert``` function below to convert the entire wikipedia dataset into JSONL files. We suggest seperating it into several JSONL files (e.g., a maximum of 1 milions lines per file, if the wikipedia tsv file has 3.000.000 lines, then you should convert it into 3 jsonl files, each jsonl file has 1.000.000 lines) to speed up the indexing process later.\n",
        "\n",
        "Note: If you use the ```pandas``` library to read the TSV file, we suggest using the ```chunksize``` argument to read the TSV file in chunks. This file is relatively large and you may not be able load it completely into RAM.\n",
        "You can learn more [here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `convert_to_jsonl(tsv_file: str, output_dir: str) -> None` below takes the Wikipedia TSV corpus as input and writes it out as one or more JSONL files in the specified directory.  \n",
        "\n",
        "- **Each JSONL line must contain a document with fields:** `id`, `contents`, and `wikipedia_id`.  \n",
        "- Some suggestions on the preprocessing part:\n",
        "    - **Split the output into multiple files** (e.g., 1,000,000 lines per file) to make later indexing faster.  \n",
        "    - **Use chunked reading** (e.g., with `pandas.read_csv(..., chunksize=...)`)  The corpus file is relatively large and you may not be able load it completely into RAM. You can learn more [here](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)."
      ],
      "metadata": {
        "id": "s8iec-mbBkdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oez-2WcyPPC"
      },
      "outputs": [],
      "source": [
        "def convert_to_jsonl(tsv_file: str, output_dir: str):\n",
        "    \"\"\"\n",
        "    Converts a large Wikipedia TSV file into one or more JSONL files.\n",
        "\n",
        "    Args:\n",
        "        tsv_file: Path to the input TSV file. Expected columns: 'id', 'wikipedia_title', 'text', 'wikipedia_id'.\n",
        "        output_dir: Directory where the JSONL files will be written. Created if it does not exist.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "            - Writes JSONL file(s) (e.g., docs01.jsonl, docs02.jsonl, ...) into output_dir.\n",
        "\n",
        "    Notes:\n",
        "        - Each JSON object should include 'id', 'contents' (title + text), and 'wikipedia_id'.\n",
        "    \"\"\"\n",
        "\n",
        "    #########\n",
        "    ##\n",
        "    ## Implement the function here\n",
        "    ##\n",
        "    #########\n",
        "\n",
        "\n",
        "\n",
        "convert_to_jsonl(tsv_file=corpus_path, output_dir=jsonl_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_jsonl = {\"id\": 0, \"contents\": \"Academy Award for Best Production Design Academy Award for Best Production Design\\nThe Academy Award for Best Production Design recognizes achievement for art direction in film. The category's original name was Best Art Direction, but was changed to its current name in 2012 for the 85th Academy Awards. This change resulted from the Art Director's branch of the Academy of Motion Picture Arts and Sciences (AMPAS) being renamed the Designer's branch. Since 1947, the award is shared with the set decorator(s). It is awarded to the best interior design in a film.\", \"wikipedia_id\": 316}\n",
        "\n",
        "print(\"Expected Output Format:\\n\" + json.dumps(sample_jsonl, indent=4))\n"
      ],
      "metadata": {
        "id": "fpXNJ0ixDzb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PLHAnlq9Xrx"
      },
      "source": [
        "We also provide a sample jsonl file [here](https://drive.google.com/file/d/1G6MOp2gIYERI4CvZ3jb9IiUJV83CsOyJ/view?usp=sharing).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YluM87Vp7HHG"
      },
      "source": [
        "## 1.2 Indexing Dataset (6 points)\n",
        "\n",
        "Next you need to call the Pyserini indexer function. Note that the index needs to store raw documents for efficient access in the later steps of the assignment. More information on Pyserini indexer can be found [here](https://github.com/castorini/pyserini/blob/master/docs/usage-index.md). You can run python commands in Google Colab by adding ```!``` before command, e.g. ```!python```.\n",
        "\n",
        "**Important notes**\n",
        "*   The output index files are relatively large, so be sure to save them on your Google Drive for easy access later.\n",
        "\n",
        "*   You can use your UMass Google Drive.\n",
        "\n",
        "\n",
        "\n",
        "*  In our experiments, we used the TPU runtime and set the number of threads to 8; the indexing step was completed in less than 5 minutes.\n",
        "For changing the runtime option, refer to [section below](#change-runtime).\n",
        "\n",
        "* Using the default `StandardAnalyzer` is fine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJL3WXZyY22"
      },
      "outputs": [],
      "source": [
        "if os.path.isdir(index_dir) and os.listdir(index_dir):\n",
        "    print(f'\"{index_dir}\" is not empty, found processed files.')\n",
        "else:\n",
        "    # Call the Pyserini indexer function here\n",
        "    #########\n",
        "    ##\n",
        "    ## Enter your code here\n",
        "    ##\n",
        "    #########\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Read Index Statistics (4 points)\n",
        "\n",
        "You need to report the total number of terms in the collection using the built inverted index\n"
      ],
      "metadata": {
        "id": "PrYjxnC3Ro2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.index.lucene import LuceneIndexReader\n",
        "import itertools\n",
        "\n",
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########\n"
      ],
      "metadata": {
        "id": "wXViPdB-tFow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNPcfZGs8URY"
      },
      "source": [
        "# 2. BM25 Retrieval (15 Points)\n",
        "\n",
        "In this part, you need to run the BM25 model to retrieve documents (at the passage level) from the Wikipedia dataset that was indexed in the previous part.\n",
        "\n",
        "The [Pyserini Interactive Searching](https://github.com/castorini/pyserini/blob/master/docs/usage-interactive-search.md) can be used to retrieve documents with respect to these queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Queries**\n",
        "\n",
        "Each example of the [KILT tasks](https://huggingface.co/datasets/facebook/kilt_tasks) has three main parts: ```id```, ```input```, and ```output```. The ```output``` part contains several outputs for the ```input```, and each of them has a ```provenance``` field. The provenance field has a list of features (```wikipedia_id```, ```start_paragraph_id```, ```end_paragraph_id```) representing Wikipedia documents and paragraphs that support the answer.\n",
        "\n",
        "In this part, you need to use ```input``` values as queries."
      ],
      "metadata": {
        "id": "oJMpcJCZ7rBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1. SimpleSearcher\n",
        "\n",
        "For retrieval, you first need to load the built index and set the parameters of the BM25."
      ],
      "metadata": {
        "id": "kFS34vsU4pd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "k1 = 1.2\n",
        "b = 0.75\n",
        "# Load the index and configure search parameters\n",
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########\n"
      ],
      "metadata": {
        "id": "ADCQaMitaVWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWGZx53JNcJR"
      },
      "source": [
        "## 2.2 Perform Retrieval using BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting retrieved results**\n",
        "\n",
        "You need to implement a function that returns a list of retrieved documents along with their scores.\n",
        "\n",
        "`def search_on_hotpotqa(raw_queries: list[dict], searcher: LuceneSearcher, top_k: int,n_threads: int) -> dict[str, dict[str, float]]`\n",
        "\n",
        "- **Expected Output Format.** The expected format of the output would be,\n",
        "```\n",
        "{\n",
        "    'query_id1': {\n",
        "        'doc_id1': <doc_ids1 score>,\n",
        "        'doc_id2': <doc_ids2 score>,\n",
        "        ...\n",
        "    },\n",
        "    ...\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "QN5ZS0Na8FBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Search**\n",
        "\n",
        "\n",
        "To speed up searching, you can use ```batch_search``` function:\n",
        "```\n",
        "hits = searcher.batch_search(queries, qids=qids, k=top_n, threads=threads)\n",
        "```\n",
        "where:\n",
        "*   queries: list of content of queries\n",
        "*   qids: list of ids of queries\n",
        "*   top_n: number of retrieved documents\n",
        "*   threads: number of threads"
      ],
      "metadata": {
        "id": "qgpG3eo4kYjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_on_hotpotqa(\n",
        "    raw_queries: list[dict],\n",
        "    searcher: LuceneSearcher,\n",
        "    top_k: int,\n",
        "    num_worker: int\n",
        ") -> dict[str, dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Runs BM25 retrieval with Pyserini for a subset of HotpotQA queries.\n",
        "\n",
        "    Args:\n",
        "        raw_queries: A dict where each item is a query object.\n",
        "        searcher: A Pyserini LuceneSearcher already pointing to the built index.\n",
        "        top_k: Number of documents to retrieve per query.\n",
        "        num_worker: Number of worker threads used by batch_search.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping query_id -> {doc_id: score}, where score is the BM25 score\n",
        "        returned by Pyserini for that (query, doc) pair.\n",
        "    \"\"\"\n",
        "\n",
        "    #########\n",
        "    ##\n",
        "    ## Implement the function here\n",
        "    ##\n",
        "    #########\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "top_k = 10\n",
        "num_worker = 2\n",
        "hotpotqa_ranklists = search_on_hotpotqa(raw_queries=raw_queries, searcher=searcher, top_k=top_k, num_worker=num_worker)"
      ],
      "metadata": {
        "id": "8hMqVq_Ylt_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_query_id = \"5ac26eed55429951e9e685bf\"\n",
        "expected_query_ranklists = {'15414678': 12.358499526977539, '14271190': 11.598899841308594, '28925671': 10.694299697875977, '1886921': 9.937000274658203, '5029879': 9.927900314331055, '32084178': 9.650400161743164, '20646629': 9.559900283813477, '19776349': 9.37909984588623, '10753001': 9.091699600219727, '3173407': 9.02079963684082}\n",
        "print(f\"Expected Output:\\n\" + json.dumps({sample_query_id: expected_query_ranklists}, indent=4))"
      ],
      "metadata": {
        "id": "v11JG6TVtJJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URMK3277Wcfl"
      },
      "source": [
        "# 3. BM25 Evaluation (10 Points)\n",
        "\n",
        "In this part, we want to measure the quality of the retrieved resutls by the BM25 model in terms of precision and MAP metrics.\n",
        "\n",
        "One tool to be used for this part is [pytrec_eval](https://github.com/cvangysel/pytrec_eval).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_59KrLYXvjIo"
      },
      "source": [
        "## Evaluating retrieved results\n",
        "\n",
        "You need to implement the function `eval_on_hotpotqa(qrels: dict, ranklists: dict) -> dict[str, float]` which evaluates the quality of BM25 retrieval results in terms of precision, recall, and MAP, each at cutoffs `3`, `5`, and `10`.\n",
        "\n",
        "- **Expected Output Format:** The returned dictionary contains the average scores across all queries in the set:\n",
        "```\n",
        "{\n",
        "  \"P_3\": value,\n",
        "  \"P_5\": value,\n",
        "  \"P_10\": value,\n",
        "  \"R_3\": value,\n",
        "  \"R_5\": value,\n",
        "  \"R_10\": value,\n",
        "  \"MAP_3\": value,\n",
        "  \"MAP_5\": value,\n",
        "  \"MAP_10\": value,\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytrec_eval\n",
        "\n",
        "def eval_on_hotpotqa(qrels: dict, ranklists: dict) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate BM25 retrieval results using pytrec_eval.\n",
        "\n",
        "    Args:\n",
        "        qrels: A dictionary of relevance judgments in TREC format\n",
        "               { query_id: { doc_id: relevance_label } }.\n",
        "        results: A dictionary of retrieval results in TREC format\n",
        "             { query_id: { doc_id: score } }.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing average P@3, P@5, P@10, MAP@3, MAP@5, MAP@10\n",
        "        across all queries.\n",
        "    \"\"\"\n",
        "\n",
        "    #########\n",
        "    ##\n",
        "    ## Implement the function here\n",
        "    ##\n",
        "    #########\n",
        "\n",
        "\n",
        "\n",
        "    return {} # You will return something meaningful before this statement\n",
        "\n",
        "\n",
        "hotpotqa_eval_results = eval_on_hotpotqa(qrels=qrels, ranklists=hotpotqa_ranklists)\n",
        "print(json.dumps(hotpotqa_eval_results, indent=4))"
      ],
      "metadata": {
        "id": "YY5W2N-jOJKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn6zfJ5Mqt0o"
      },
      "source": [
        "# 4. Retrieval-Augmented Generation (RAG) (20 Points)\n",
        "\n",
        "We use retrieval-augmented generation (RAG) to describe a pipeline that retrieves a set of relevant documents and passes them to an LLM to produce answers to questions.\n",
        "\n",
        "In this assignment, we use `Qwen/Qwen2.5-3B` as the backbone LLM and perform RAG with the top-$k$ BM25 results to answer questions from the HotpotQA dataset.\n",
        "\n",
        "After generating answers, we evaluate their quality with [Exact Match (EM)](https://huggingface.co/spaces/evaluate-metric/exact_match), which checks whether a prediction exactly matches the gold answer.\n",
        "\n",
        "Most of the components are implemented below and you only need to run them and provide your BM25 results.\n",
        "\n",
        "Your task is to\n",
        "- prepare the prompts for the LLM, and\n",
        "- analyze how the quality of BM25 retrieval correlates with the quality of the generated answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYKOxpMVrgHx"
      },
      "source": [
        "## 4.1 Inverted index for constructing LLM prompts (6 Points)\n",
        "\n",
        "Before we issue the query to the LLM, we need to first prepare the prompts for the LLM based on the retrieval results from our previous steps. Similar to other retrieval/search procedures, we need to have a lookup function which maps from doc ID to the actual doc content.\n",
        "\n",
        "You would need to implement the function `get_ctxs(doc_ids: dict[str, float], searcher: LuceneSearcher) -> list[dict[str, str | float]]` which gets the content of the top-ranked documents from the Pyserini index.\n",
        "\n",
        "\n",
        "- **Expected Output Format.** The expected format of the output is,\n",
        "```\n",
        "[\n",
        "  {\n",
        "    \"id\": <doc_id>,\n",
        "    \"text\": <document contents>,\n",
        "    \"bm25_score\": <bm25 score>\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ctxs(doc_ids: dict[str, float], searcher: LuceneSearcher) -> list[dict[str, str | float]]:\n",
        "    \"\"\"\n",
        "    Get document contents for given doc_ids from the Pyserini index.\n",
        "\n",
        "    Args:\n",
        "        doc_ids: A dictionary having document IDs and their BM25 scores.\n",
        "        searcher: A Pyserini LuceneSearcher object pointing to the built index.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each containing:\n",
        "            - \"id\": the document ID\n",
        "            - \"text\": the document contents\n",
        "            - \"bm25_score\": the BM25 score\n",
        "        The list is sorted by BM25 score in descending order.\n",
        "    \"\"\"\n",
        "\n",
        "    #########\n",
        "    ##\n",
        "    ## Implement the function here\n",
        "    ##\n",
        "    #########\n",
        "\n",
        "    return {} # You will return something meaningful before this statement\n",
        "\n"
      ],
      "metadata": {
        "id": "jmAfsEdl8l8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_doc_ids = {\"15414678\": 12.358499526977539, \"14271190\": 11.598899841308594}\n",
        "expected_sample_ctxs = [{'id': '15414678', 'text': \"Purcell, Kansas Purcell, Kansas\\nPurcell is an unincorporated community in Doniphan County, Kansas, United States. It is located east of Everest, south of K-20, on highway K-137.\\nHistory.\\nPurcell was founded about 1886. John Purcell was one of the earliest settlers.\\nA post office was opened in Purcell in 1887, and remained in operation until it was discontinued in 1956.\\nSt. Mary's Catholic Church, which is listed on the National Register of Historic Places, is located in Purcell.\", 'bm25_score': 12.358499526977539}, {'id': '14271190', 'text': 'McClain County, Oklahoma to the northwest. It ran through Byars and Purcell, and established Washington, Cole, and Blanchard.\\nPurcell was a starting point for the Land Run of 1889. It also was at the dividing line between Indian Territory, where alcohol could not be sold, and Oklahoma Territory, where alcohol sale was legal. The town of Lexington, across the river from Purcell, had numerous saloons. In 1899, the Purcell Bridge Company built a toll bridge across the river, profiting from the alcohol trade.\\nGeography.', 'bm25_score': 11.598899841308594}]\n",
        "\n",
        "sample_ctxs = get_ctxs(doc_ids=sample_doc_ids, searcher=searcher)\n",
        "\n",
        "print(f\"Expected Output:\\n\" + json.dumps(expected_sample_ctxs, indent=4))\n",
        "print(f\"    Your Output:\\n\" + json.dumps(sample_ctxs, indent=4))"
      ],
      "metadata": {
        "id": "g1TlWDR5B8cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `get_ctx()` function above, the top retrieved documents along with their contents are written in a JSON file. This file is used for the RAG experiments in the next step so that you don't need to get the retrieved results more than one time.\n",
        "\n",
        "The format of JSON file is:\n",
        "```\n",
        "[{\n",
        "  \"id\": \"query_1\",\n",
        "  \"input\": \"the first input\",\n",
        "  \"answers\": ['answer 1', 'answer 2',...]\n",
        "  \"ctxs\": [{\"id\": \"paragraph1 id\"\n",
        "            \"text\": \"paragraph1 content\"},\n",
        "            {\"id\": \"paragraph2 id\"\n",
        "            \"text\": \"paragraph2 content\"},\n",
        "            ...\n",
        "          ]     \n",
        "},\n",
        "  {\n",
        "  \"id\": \"query_2\",\n",
        "  \"input\": \"the second input\",\n",
        "  \"answers\": ['answer 1', 'answer 2',...]\n",
        "  \"ctxs\": [{\"id\": \"paragraph1 id\"\n",
        "            \"text\": \"paragraph1 content\"},\n",
        "            {\"id\": \"paragraph2 id\"\n",
        "            \"text\": \"paragraph2 content\"},\n",
        "            ...\n",
        "          ]     \n",
        "},\n",
        "...\n",
        "]\n",
        "```\n",
        "where \"id\", \"input\", and \"answers\" are obtained from the HotpotQA dataset of KILT, and \"ctxs\" contains the document content."
      ],
      "metadata": {
        "id": "R0IjbRSfJ18b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT MODIFY THE CODE BELOW!!\n",
        "\n",
        "def prepare_json_file(raw_queries: dict, ranklists: dict, searcher: LuceneSearcher, json_cache_path: str):\n",
        "    saved_json_list = []\n",
        "    for entry in tqdm(raw_queries, desc=\"Prepare LLM Prompt\", unit=\" Query\"):\n",
        "        saved_dict = {'answers': []}\n",
        "        saved_dict['id'] = entry['id']\n",
        "        saved_dict['input'] = entry['input']\n",
        "        for output in entry['output']:\n",
        "            saved_dict['answers'].append(output['answer'])\n",
        "        saved_dict['ctxs'] = get_ctxs(ranklists[entry['id']], searcher)\n",
        "        saved_json_list.append(saved_dict)\n",
        "    with open(json_cache_path, 'w', encoding=\"utf-8\") as f_out:\n",
        "        json.dump(saved_json_list, f_out, indent=4)\n",
        "    print(f\"Processed LLM Prompts are saved at '{json_cache_path}'.\")\n",
        "\n",
        "\n",
        "prepare_json_file(raw_queries=raw_queries, ranklists=hotpotqa_ranklists, searcher=searcher, json_cache_path=json_cache_path)"
      ],
      "metadata": {
        "id": "Vy52m2YaO8gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Prepare the prompts for LLM inference\n",
        "\n"
      ],
      "metadata": {
        "id": "s8jg5stV-q3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(No Implementation Needed!)**\n",
        "\n",
        "We provide a set of utility functions which help to build the prompts for RAG evaluation. Part of the code is adopted from [here](https://github.com/AI21Labs/in-context-ralm/)."
      ],
      "metadata": {
        "id": "2jA3UmsFDBP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN2itkAHNRwV"
      },
      "outputs": [],
      "source": [
        "# Part of this code is adopted from https://github.com/AI21Labs/in-context-ralm/\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def normalize_question(question):\n",
        "  if not question.endswith(\"?\"):\n",
        "      question = question + \"?\"\n",
        "  return question[0].lower() + question[1:]\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "  def remove_articles(text):\n",
        "      return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "  def white_space_fix(text):\n",
        "      return \" \".join(text.split())\n",
        "  def remove_punc(text):\n",
        "      exclude = set(string.punctuation)\n",
        "      return \"\".join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "      return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def text_has_answer(answers, text) -> bool:\n",
        "  if isinstance(answers, str):\n",
        "      answers = [answers]\n",
        "  text = normalize_answer(text)\n",
        "  for single_answer in answers:\n",
        "      single_answer = normalize_answer(single_answer)\n",
        "      if single_answer in text:\n",
        "          return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def exact_match(prediction, ground_truth):\n",
        "  return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "\n",
        "def get_answer_from_model_output(outputs, tokenizer, prompt):\n",
        "  generation_str = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
        "  generation_str = generation_str[len(prompt):]\n",
        "  answer = generation_str.split(\"\\n\")[0]\n",
        "  return answer, generation_str\n",
        "\n",
        "\n",
        "def load_dataset(dataset_path):\n",
        "    print(\"Loading dataset:\", dataset_path)\n",
        "    with open(dataset_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def evaluate_dataset(model, tokenizer, device, eval_dataset, max_length, \\\n",
        "                     num_docs=0, output_dir=None, max_tokens_to_generate=10):\n",
        "    idx = 0\n",
        "    num_correct = 0\n",
        "    num_has_answer = 0\n",
        "    num_too_long = 0\n",
        "    sample_prompt = None\n",
        "    for ex in (tq := tqdm(eval_dataset, desc=f\"EM:  0.0%\")):\n",
        "        answers = ex[\"answers\"]\n",
        "        prompt = build_qa_prompt(ex, num_docs=num_docs)\n",
        "        if idx == 0:\n",
        "            sample_prompt = prompt\n",
        "        has_answer = text_has_answer(answers, prompt)\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "        if input_ids.shape[-1] > max_length - max_tokens_to_generate:\n",
        "            num_too_long += 1\n",
        "            input_ids = input_ids[..., -(max_length - max_tokens_to_generate):]\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids, max_new_tokens=max_tokens_to_generate, pad_token_id=tokenizer.eos_token_id)\n",
        "        prediction, generation = get_answer_from_model_output(outputs, tokenizer, prompt)\n",
        "        is_correct = any([exact_match(prediction, answer) for answer in answers])\n",
        "        idx += 1\n",
        "        if is_correct:\n",
        "            num_correct += 1\n",
        "        if has_answer:\n",
        "            num_has_answer += 1\n",
        "        tq.set_description(f\"EM: {num_correct / idx * 100:4.1f}%\")\n",
        "    em = num_correct / idx * 100\n",
        "    has_answer = num_has_answer / idx * 100\n",
        "    print(f\"EM: {em:.1f}%\")\n",
        "    print(f\"% of prompts with answer: {num_has_answer / idx * 100:.1f}%\")\n",
        "    if output_dir is not None:\n",
        "        d = {\"em\": em, \"has_answer\": has_answer, \"num_examples\": idx, \"too_long\": num_too_long}\n",
        "        eval_output_path = os.path.join(output_dir, \"eval.json\")\n",
        "        with open(eval_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(d) + \"\\n\")\n",
        "        print(f\"The evaluation is saved at '{eval_output_path}'.\")\n",
        "        if sample_prompt is not None:\n",
        "            sample_prompt_path = os.path.join(output_dir, \"example_prompt.txt\")\n",
        "            with open(sample_prompt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(sample_prompt)\n",
        "            print(f\"The sample prompt is saved at '{sample_prompt_path}'.\")\n",
        "    return em\n",
        "\n",
        "\n",
        "def build_qa_prompt(example, num_docs=1):\n",
        "    if num_docs == 0:\n",
        "        question_text = normalize_question(example[\"input\"])\n",
        "        ex_prompt = f\"Answer the question:\\nQ: {question_text}\\nA:\"\n",
        "    elif num_docs == 1:\n",
        "        q = normalize_question(example[\"input\"])\n",
        "        text = example['ctxs'][0]['text']\n",
        "        ex_prompt = f\"{text}\\n\\nBased on this text, answer this question:\\nQ: {q}\\nA:\"\n",
        "    else:\n",
        "        q = normalize_question(example[\"input\"])\n",
        "        docs_text = \"\\n\\n\".join([f\"{ctx['text']}\" for ctx in example[\"ctxs\"][:num_docs]])\n",
        "        ex_prompt = f\"{docs_text}\\n\\nBased on these texts, answer this question:\\nQ: {q}\\nA:\"\n",
        "    return ex_prompt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Load the LLM\n",
        "<a name=\"change-runtime\"></a>\n",
        "**(No Implementation Needed!)**\n",
        "\n",
        "We recommend using a GPU for LLM inference. In Colab, go to `Runtime → Change runtime type` and select a **GPU** (we suggest a **T4**).\n",
        "\n",
        "**Note.** A T4 GPU typically finishes inference for ~200 queries in a few minutes. CPU-only runs will take much longer. You can also explore TPU for faster inference, though it may require additional setup and library support."
      ],
      "metadata": {
        "id": "0ikYTy6uFmQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPGNOAxQQ-tn"
      },
      "outputs": [],
      "source": [
        "# Loading the model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "config = AutoConfig.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5UPeWA9sNQP"
      },
      "source": [
        "## 4.4 Run the RAG Pipeline (4 Points)\n",
        "\n",
        "In this part, you first get the quality of generated answers by the LLM when retrieved results are used to augment the prompts.\n",
        "\n",
        "One aspect to study is how varying the number of retrieved results passed to the LLM impacts the quality of its generated answers. For this purpose, you need to report the Exact Match (EM) scores in **four different settings** when\n",
        "\n",
        "\n",
        "*   no document is passed to the the LLM: the LLM prompt only contains the question.\n",
        "*   top-3 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-3 retrieved documents.\n",
        "*   top-5 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-5 retrieved documents.\n",
        "*   top-10 retrieved documents by BM25 are passed to LLM: the LLM prompt  contains the question and the top-10 retrieved documents.\n",
        "\n",
        "To get these performance values, you need to call the\n",
        "`run_eval()` function below with the different values for the parameter `top_k`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval(model, tokenizer, config, prepared_prompts_file, output_dir, top_k=10, dataset_type='QA') -> float:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    eval_dataset = load_dataset(prepared_prompts_file)\n",
        "    model_max_length = config.n_positions if hasattr(config, \"n_positions\") else config.max_position_embeddings\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return evaluate_dataset(model, tokenizer, device, eval_dataset, model_max_length, num_docs=top_k, output_dir=output_dir)"
      ],
      "metadata": {
        "id": "e4-AkYmQaLvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.1 Experiments on `top_k=0`"
      ],
      "metadata": {
        "id": "KEbBxLPFHhZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########\n"
      ],
      "metadata": {
        "id": "An0pmVzzPyIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.2 Experiments on `top_k=3`"
      ],
      "metadata": {
        "id": "8CDEjL3OIJmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########"
      ],
      "metadata": {
        "id": "zk-503RdIE3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.3 Experiments on `top_k=5`"
      ],
      "metadata": {
        "id": "IYtzCsH1ILvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########\n"
      ],
      "metadata": {
        "id": "bEc2xPCsIGRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.4 Experiments on `top_k=10`"
      ],
      "metadata": {
        "id": "kDhTh5fBINws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########\n",
        "##\n",
        "## Enter your code here\n",
        "##\n",
        "#########\n"
      ],
      "metadata": {
        "id": "zrZqZFuEIHUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.5 Fill out the following table using the performance obtained above. (4 Points)"
      ],
      "metadata": {
        "id": "ynGJrZ4sQh75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| top_k |   Exact Match   |\n",
        "|:-----:|:----------------|\n",
        "|   0   |                 |\n",
        "|   3   |                 |\n",
        "|   5   |                 |\n",
        "|  10   |                 |"
      ],
      "metadata": {
        "id": "olUqNgNrQojl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxSLOA2d2dX"
      },
      "source": [
        "## 4.5 Performance Analysis (10 Points)\n",
        "\n",
        "Answer these questions based on the performance results you obtained above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5.1 Does passing more documents to the LLM lead to higher quality of generated answers? (3 Points)\n"
      ],
      "metadata": {
        "id": "WmgxOKyVI2QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "_SZP60CKI57_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ErOHjShXw3Y"
      },
      "source": [
        "### 4.5.2. Does the setting with the highest precision among the four settings above result in the highest exact-match score? (3 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "vuyGXwRXJEyU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_MzocrEX3mQ"
      },
      "source": [
        "### 4.5.3. Is there a relationship between the two evaluation metrics, the precision of the retreived results and the exact-match score of the generated answers? (4 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "fFf5yTcWJJeE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSZ4ImHxJlj5"
      },
      "source": [
        "# 5. Extra Credits (Max 10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Prompt Engineering\n",
        "You can change the prompt template in the ```build_prompt``` function above and study how that impacts the quality of the answers generated by the LLM. For example, you may:\n",
        "- Ask the LLM to reason step-by-step before providing an answer.\n",
        "- Design a prompt that encourages the LLM to produce a concise, exact answer.\n",
        "- Include few-shot examples in the prompt before generation."
      ],
      "metadata": {
        "id": "NBpSDI8ULLKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "eebEoD4JLaHK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_xxTGaPqqlY"
      },
      "source": [
        "## 5.2 Position bias\n",
        "You can study how the rank of the relevant document (the one containing the exact-match answer) impacts the quality of the answers generated by the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "MT951goDLbDV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D_RznKg1hAp"
      },
      "source": [
        "## 5.3 Context length\n",
        "You can investigate how constructing LLM prompts with exactly one relevant document and a varying number of noise documents (up to the maximum you can include) impacts the quality of the answers generated by the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "08fgZoxkLbwH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcceXxmQ1ky6"
      },
      "source": [
        "## 5.4 LLM Variants\n",
        "\n",
        "You can also experiment with different model sizes in the Qwen2.5 family (e.g., 1.5B, 3B, 7B) and compare the instruct vs. non-instruct variants. For example, the instruct model can be accessed as Qwen/Qwen2.5-3B-instruct. Note that you may need to apply quantization when running the 7B model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "eFsNzsILLcgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. AI Disclosure\n",
        "\n",
        "*   Did you use any AI assistance to complete this assignment? If so, please also specify what AI you used.\n",
        "    * *your response here*\n",
        "\n",
        "\n",
        "---\n",
        "*(only complete the below questions if you answered yes above)*\n",
        "\n",
        "*   If you used a large language model to assist you, please paste *all* of the prompts that you used below. Add a separate bullet for each prompt, and specify which problem is associated with which prompt.\n",
        "\n",
        "    * *your response here*\n",
        "\n",
        "\n",
        "*   **Free response**: For each problem for which you used assistance, describe your overall experience with the AI. How helpful was it? Did it just directly give you a good answer, or did you have to edit it? Was its output ever obviously wrong or irrelevant? Did you use it to get the answer or check your own answer?\n",
        "    * *your response here*\n"
      ],
      "metadata": {
        "id": "rEZzpI2SLvs7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}